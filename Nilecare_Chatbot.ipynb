{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rafia-10/Nilecare_AI_Chatbot/blob/main/Nilecare_Chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import subprocess\n",
        "import time\n",
        "import requests"
      ],
      "metadata": {
        "id": "rJ4L4X3BbuZU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Clone Repo & CD ---\n",
        "!git clone 'https://github.com/rafia-10/Nilecare_AI_Chatbot.git'\n",
        "%cd 'Nilecare_AI_Chatbot'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6enwhBw1b62y",
        "outputId": "ca13e3de-1cbb-4d4f-d0b9-b4f156301dca"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Nilecare_AI_Chatbot'...\n",
            "remote: Enumerating objects: 33, done.\u001b[K\n",
            "remote: Counting objects: 100% (33/33), done.\u001b[K\n",
            "remote: Compressing objects: 100% (26/26), done.\u001b[K\n",
            "Receiving objects: 100% (33/33), 7.85 KiB | 7.85 MiB/s, done.\n",
            "Resolving deltas: 100% (7/7), done.\n",
            "remote: Total 33 (delta 7), reused 30 (delta 4), pack-reused 0 (from 0)\u001b[K\n",
            "/content/Nilecare_AI_Chatbot\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python main.py"
      ],
      "metadata": {
        "id": "tkRPrxGFuttH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b448346d-d49f-44c4-d212-1232caea983d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-08-20 21:30:49.981245: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1755725450.001500    3377 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1755725450.008822    3377 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1755725450.025051    3377 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1755725450.025077    3377 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1755725450.025080    3377 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1755725450.025085    3377 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-08-20 21:30:50.029680: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Starting NileCare Chatbot with model: Qwen/Qwen1.5-1.8B-Chat\n",
            "Initializing Chatbot with LLM model: Qwen/Qwen1.5-1.8B-Chat\n",
            "Loading Hugging Face model: Qwen/Qwen1.5-1.8B-Chat\n",
            "tokenizer_config.json: 1.29kB [00:00, 6.50MB/s]\n",
            "vocab.json: 2.78MB [00:00, 64.7MB/s]\n",
            "merges.txt: 1.67MB [00:00, 106MB/s]\n",
            "tokenizer.json: 7.03MB [00:00, 117MB/s]\n",
            "config.json: 100% 662/662 [00:00<00:00, 5.44MB/s]\n",
            "model.safetensors: 100% 3.67G/3.67G [00:43<00:00, 84.8MB/s]\n",
            "generation_config.json: 100% 206/206 [00:00<00:00, 2.19MB/s]\n",
            "Device set to use cuda:0\n",
            "Hugging Face model loaded successfully.\n",
            "\n",
            "--- Welcome to NileCare Chatbot ---\n",
            "Type 'exit' to quit the conversation.\n",
            "Type 'reset' to clear the current conversation history and start fresh.\n",
            "Type your message below and press Enter.\n",
            "-----------------------------------\n",
            "You: hello\n",
            "Chatbot: Hello! How can I assist you today? Is there something specific you would like to know or discuss? I'm here to answer any questions you may have on a wide range of topics, from general knowledge to more specialized subjects. Just let me know how I can be of help.\n",
            "You: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mSkfxqlslyju"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}